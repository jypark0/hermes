# @package _global_

train:
  epochs: 100
  batch_size: 2
  lr: 7e-4

optimizer:
  _target_: torch.optim.Adam
  lr: ${train.lr}

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${train.epochs}

# params: 165,013
backbone:
  net:
    # 2 blocks
    block_dims:
      - ${dataset.in_dim}
      - 7
      - 7
      - 7
      - 7
    block_orders: [1, 2, 2, 2, 1] # linear layers at the end
    final_activation: false
    node_batch_size: null
    n_heads: 1

model:
  post_process_dims:
    - 7 # rho0 only
    - ${dataset.out_dim}
